{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c11285-e8ed-4e6e-bfec-9d38c0818d55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb491d50-bf2d-46e4-ab0d-a9ec6c558bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([1,3,45,6,7,7,89,7])\n",
    "image_token_id =torch.tensor([7])\n",
    "image_mask = input_ids == image_token_id\n",
    "image_embeds = torch.tensor([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31a1e2b-5fd3-4ca7-bd24-5c0941bfe2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False,  True,  True, False,  True])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5f612f-f2d9-4141-9cf3-a046a96f77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[image_mask] = image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f224b7a-7162-4257-922c-e5b5dd3bcf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   3,  45,   6, 100, 100,  89, 100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364287d5-490c-43ad-abfc-ec3dbf133852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_bidirectional_attention_mask(\n",
    "    attention_mask: torch.Tensor,\n",
    "    sequence_length = 3,\n",
    "    target_length = 4,\n",
    "    dtype = torch.float32,\n",
    "    device = \"cpu\",\n",
    "    batch_size = 2\n",
    "):\n",
    "    min_dtype = torch.finfo(dtype).min,\n",
    "    if attention_mask is not None and attention_mask.dim() == 4:\n",
    "        bidirectional_mask = attention_mask\n",
    "    else:\n",
    "        # 创建全连接的基础掩码\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        bidirectional_mask = torch.ones((batch_size, 1, sequence_length, target_length),dtype=dtype,device=device)\n",
    "        \n",
    "        # 处理 padding 掩码\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                padding_mask = attention_mask[:, None, None, :].to(device)\n",
    "                padding_mask = (1.0 - padding_mask) * min_dtype\n",
    "                bidirectional_mask = bidirectional_mask + padding_mask\n",
    "                \n",
    "    return bidirectional_mask\n",
    "# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n",
    "def _prepare_4d_bi_attention_mask_with_cache_position(\n",
    "    attention_mask: torch.Tensor,\n",
    "    sequence_length = 3,\n",
    "    target_length = 4,\n",
    "    dtype = torch.float32,\n",
    "    device = \"cpu\",\n",
    "    batch_size = 2\n",
    "):\n",
    "    min_dtype = torch.finfo(dtype).min,\n",
    "    if attention_mask is not None and attention_mask.dim() == 4:\n",
    "        bidirectional_mask = attention_mask\n",
    "    else:\n",
    "        bidirectional_mask = torch.zeros((sequence_length, target_length), dtype=dtype, device=device)\n",
    "        bidirectional_mask = bidirectional_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            bidirectional_mask = bidirectional_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "            mask_length = attention_mask.shape[-1]\n",
    "            padding_mask = attention_mask[:, None, None, :] == 0\n",
    "            bidirectional_mask[:, :, :, :mask_length] = bidirectional_mask[:, :, :, :mask_length].masked_fill(\n",
    "                padding_mask, min_dtype\n",
    "            )\n",
    "\n",
    "    return bidirectional_mask\n",
    "# 输入参数\n",
    "batch_size = 2\n",
    "sequence_length = 3  # 当前输入序列长度\n",
    "target_length = 4    # 总序列长度（含缓存）\n",
    "dtype = torch.float32\n",
    "device = \"cpu\"\n",
    "min_dtype = torch.finfo(dtype).min  # -3.4028e+38\n",
    "\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 0],  # 样本1：位置0-2中前两个有效\n",
    "    [1, 0, 0]   # 样本2：位置0有效\n",
    "], dtype=torch.long)\n",
    "_prepare_bidirectional_attention_mask(attention_mask)\n",
    "_prepare_4d_bi_attention_mask_with_cache_position(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda61c7a-6c20-42cc-8e92-d6852af350ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "282ac932-e607-4853-b219-e3ba2025922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding_mask tensor([[[[False,  True, False,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False,  True,  True]]]])\n",
      "bidirectional_mask4 tensor([[[[ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38]]]])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38]]]])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00, -3.4028e+38,  0.0000e+00, -3.4028e+38]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38]]]])\n",
      "tensor([[[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def _prepare_bidirectional_attention_mask(\n",
    "    attention_mask: torch.Tensor,\n",
    "    sequence_length = 3,\n",
    "    target_length = 3,\n",
    "    dtype = torch.float32,\n",
    "    device = \"cpu\",\n",
    "    batch_size = 2\n",
    "):\n",
    "    min_dtype = torch.finfo(dtype).min\n",
    "    if attention_mask is not None and attention_mask.dim() == 4:\n",
    "        bidirectional_mask = attention_mask\n",
    "    else:\n",
    "        # 创建全连接的基础掩码\n",
    "        bidirectional_mask = torch.zeros((batch_size, 1, sequence_length, target_length), dtype=dtype, device=device)\n",
    "\n",
    "        # 处理 padding 掩码\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                padding_mask = attention_mask[:, None, None, :].to(device)\n",
    "                padding_mask = (1.0 - padding_mask) * min_dtype\n",
    "                # 创建一个与 bidirectional_mask 形状相同的全零张量\n",
    "                full_padding_mask = torch.zeros_like(bidirectional_mask)\n",
    "                # 将 padding_mask 的值复制到 full_padding_mask 的相应位置\n",
    "                full_padding_mask[:, :, :, :padding_mask.shape[-1]] = padding_mask\n",
    "                bidirectional_mask = bidirectional_mask + full_padding_mask\n",
    "\n",
    "    return bidirectional_mask\n",
    "\n",
    "# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n",
    "def _prepare_4d_bi_attention_mask_with_cache_position(\n",
    "    attention_mask: torch.Tensor,\n",
    "    sequence_length = 3,\n",
    "    target_length = 3,\n",
    "    dtype = torch.float32,\n",
    "    device = \"cpu\",\n",
    "    batch_size = 2\n",
    "):\n",
    "    min_dtype = torch.finfo(dtype).min\n",
    "    if attention_mask is not None and attention_mask.dim() == 4:\n",
    "        bidirectional_mask = attention_mask\n",
    "    else:\n",
    "        bidirectional_mask = torch.zeros((sequence_length, target_length), dtype=dtype, device=device)        \n",
    "        bidirectional_mask = bidirectional_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            bidirectional_mask = bidirectional_mask.clone()  # copy to contiguous memory for in-place edit            \n",
    "            mask_length = attention_mask.shape[-1]\n",
    "            padding_mask = attention_mask[:, None, None, :] == 0\n",
    "            print(\"padding_mask\",padding_mask)\n",
    "            \n",
    "            bidirectional_mask[:, :, :, :mask_length] = bidirectional_mask[:, :, :, :mask_length].masked_fill(\n",
    "                padding_mask, min_dtype\n",
    "            )\n",
    "            print(\"bidirectional_mask4\",bidirectional_mask)\n",
    "            \n",
    "    return bidirectional_mask\n",
    "\n",
    "# 输入参数\n",
    "batch_size = 2\n",
    "sequence_length = 4  # 当前输入序列长度\n",
    "target_length = 4    # 总序列长度（含缓存）\n",
    "dtype = torch.float32\n",
    "device = \"cpu\"\n",
    "min_dtype = torch.finfo(dtype).min  # -3.4028e+38\n",
    "\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 0, 1, 0],  # 样本1：位置 0-2中前两个有效\n",
    "    [1, 1, 0, 0]   # 样本2：位置 0 有效\n",
    "], dtype=torch.long)\n",
    "\n",
    "# 显式传入所有必要的参数\n",
    "result1 = _prepare_bidirectional_attention_mask(\n",
    "    attention_mask,\n",
    "    sequence_length=sequence_length,\n",
    "    target_length=target_length,\n",
    "    dtype=dtype,\n",
    "    device=device,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "result2 = _prepare_4d_bi_attention_mask_with_cache_position(\n",
    "    attention_mask,\n",
    "    sequence_length=sequence_length,\n",
    "    target_length=target_length,\n",
    "    dtype=dtype,\n",
    "    device=device,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result1-result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b446743-4622-4ae4-b01b-a061b9eca596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported models:\n",
      "  Model ID                      : HuggingFace Path\n",
      "  ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    temp = \"Model ID\"\n",
    "    ljust = 30                   # 定义一个左对齐的宽度\n",
    "    print(\"Supported models:\")   # 打印支持的模型信息的表头\n",
    "    print(f\"  {temp.ljust(ljust)}: HuggingFace Path\")\n",
    "    print(\"  ------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02341e99-00bd-42d3-97d9-1efee06940bb",
   "metadata": {},
   "source": [
    "### loss 比较实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a6c32-a48a-420c-b0e3-305623583c66",
   "metadata": {},
   "source": [
    "#### 常规 CrossEntropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99da8b5e-62ff-4ec7-bcc8-a12705a44421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_log_softmax(x, dim=-1):\n",
    "    \"\"\"\n",
    "    自定义实现的 log_softmax 函数\n",
    "    \n",
    "    参数:\n",
    "        x: 输入张量\n",
    "        dim: 沿着哪个维度计算 softmax\n",
    "        \n",
    "    返回:\n",
    "        与输入形状相同的 log_softmax 结果\n",
    "    \"\"\"\n",
    "    # 减去最大值以提高数值稳定性\n",
    "    x_max = x.max(dim=dim, keepdim=True).values\n",
    "    print(\"x_max:\\n\",x_max)\n",
    "    x_exp = (x - x_max).exp()\n",
    "    \n",
    "    # 计算 log(sum(exp(x)))\n",
    "    log_sum_exp = x_exp.sum(dim=dim, keepdim=True).log() \n",
    "    \n",
    "    # 计算 log_softmax\n",
    "    return (x - x_max ) - log_sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57444fd2-78bb-4fa9-8d53-7b18f5a73cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# pytorch 封装的 CrossEntropy计算得到的损失值为: 2.13299560546875\n",
    "\n",
    "# 自定义实现 CrossEntropyLoss\n",
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, cos_sim, nce_labels):\n",
    "        # cos_sim形状: (b, n)\n",
    "        # nce_labels形状: (b,)\n",
    "        # log_probs = F.log_softmax(cos_sim, dim=1)\n",
    "        log_probs = custom_log_softmax(cos_sim, dim=1)\n",
    "        \n",
    "        # 使用 gather 收集每个样本对应标签的log概率\n",
    "        # 将nce_labels的形状调整为(b, 1)，便于gather操作\n",
    "        print(\"log_probs:\\n\",log_probs)\n",
    "        selected_log_probs = log_probs.gather(1, nce_labels.unsqueeze(1))\n",
    "        print(\"selected_log_probs:\\n\",selected_log_probs)\n",
    "        \n",
    "        # 取平均损失\n",
    "        loss = -selected_log_probs.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b626d7-214e-4bf2-ae04-e4e969a00d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim:\n",
      " tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels:\n",
      " tensor([3, 0])\n",
      "x_max:\n",
      " tensor([[0.9355],\n",
      "        [0.9447]])\n",
      "log_probs:\n",
      " tensor([[-1.6656, -1.4151, -2.0605, -1.9515, -1.6035, -2.3291],\n",
      "        [-2.3145, -1.5945, -1.9917, -1.7459, -1.4353, -1.9027]])\n",
      "selected_log_probs:\n",
      " tensor([[-1.9515],\n",
      "        [-2.3145]])\n",
      "计算得到的损失值为: 2.13299560546875\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# 创建损失函数实例\n",
    "criterion = CustomCrossEntropyLoss()\n",
    "print(\"cos_sim:\\n\",cos_sim)\n",
    "print(\"nce_labels:\\n\",nce_labels)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(cos_sim, nce_labels)\n",
    "\n",
    "print(f\"计算得到的损失值为: {loss}\")\n",
    "# 计算得到的损失值为: 2.132995367050171 ---  自定义 custom_log_softmax\n",
    "# 计算得到的损失值为: 2.13299560546875  ---  标准的 F.log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b93a3f-6704-4e0f-9412-f83f52511778",
   "metadata": {},
   "source": [
    "#### 显存优化版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbcadff-d868-4506-aa7f-bacd4051bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 显存优化版本\n",
    "class MemoryEfficientCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, cos_sim, nce_labels):\n",
    "        \"\"\"\n",
    "        cos_sim: (b, n)  输入相似度矩阵\n",
    "        nce_labels: (b,) 目标位置标签\n",
    "        \"\"\"\n",
    "        # 并行提取目标位置的相似度分数 (b,)\n",
    "        target_scores = cos_sim.gather(1, nce_labels.unsqueeze(1)).squeeze(1)\n",
    "        print(\"target_scores:\",target_scores,\"\\ntarget_scores.size:\",target_scores.size())\n",
    "        \n",
    "        # 计算每个样本的logsumexp (b,)\n",
    "        logsumexp = torch.logsumexp(cos_sim, dim=1)  # 数值稳定\n",
    "        print(\"logsumexp:\",logsumexp,\"\\nlogsumexp.size:\",logsumexp.size())\n",
    "        \n",
    "        # 计算单个样本损失: logsumexp - target_score\n",
    "        loss_per_sample = logsumexp - target_scores  # 形状 (b,)\n",
    "        print(\"loss_per_sample:\",loss_per_sample,\"\\nloss_per_sample.size:\",loss_per_sample.size())\n",
    "        \n",
    "        # 最终损失均值\n",
    "        return loss_per_sample.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "760fd43e-27a8-4ab8-90c3-1a573b8b6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels: tensor([3, 0])\n",
      "target_scores: tensor([0.3991, 0.0654]) \n",
      "target_scores.size: torch.Size([2])\n",
      "logsumexp: tensor([2.3506, 2.3800]) \n",
      "logsumexp.size: torch.Size([2])\n",
      "loss_per_sample: tensor([1.9515, 2.3145]) \n",
      "loss_per_sample.size: torch.Size([2])\n",
      "计算得到的损失值为: 2.132995367050171\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# 创建损失函数实例\n",
    "criterion = MemoryEfficientCrossEntropy()\n",
    "print(\"cos_sim:\",cos_sim)\n",
    "print(\"nce_labels:\",nce_labels)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(cos_sim, nce_labels)\n",
    "\n",
    "print(f\"计算得到的损失值为: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a92abe-b4a3-4ec6-aa57-712cd70c129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_scores: tensor([1.2000, 2.1000], grad_fn=<SqueezeBackward1>) \n",
      "target_scores.size: torch.Size([2])\n",
      "logsumexp: tensor([1.7422, 2.3758], grad_fn=<LogsumexpBackward0>) \n",
      "logsumexp.size: torch.Size([2])\n",
      "loss_per_sample: tensor([0.5422, 0.2758], grad_fn=<SubBackward0>) \n",
      "loss_per_sample.size: torch.Size([2])\n",
      "显存优化损失: 0.4090, 官方损失: 0.4090\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 输入数据\n",
    "cos_sim = torch.tensor([[1.2, 0.5, -0.3], [0.8, -1.0, 2.1]], requires_grad=True)\n",
    "nce_labels = torch.tensor([0, 2])\n",
    "\n",
    "# 显存优化版本\n",
    "custom_loss = MemoryEfficientCrossEntropy()(cos_sim, nce_labels)\n",
    "\n",
    "# 官方实现\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "official_loss = ce_loss(cos_sim, nce_labels).item()\n",
    "\n",
    "# 结果一致（误差来自浮点精度）\n",
    "print(f\"显存优化损失: {custom_loss:.4f}, 官方损失: {official_loss:.4f}\") \n",
    "# 输出：显存优化损失: 0.4100, 官方损失: 0.4100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc8327-0885-4b2a-b319-bcda5cc2d1b0",
   "metadata": {},
   "source": [
    "#### 分母正样本加权版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29231b03-6856-4099-8cdf-5626c9fe04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlphaWeightedCELoss(nn.Module):\n",
    "    def __init__(self, alpha=1.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 确保alpha>0\n",
    "    def forward(self, cos_sim, labels):\n",
    "        \"\"\"\n",
    "        cos_sim: (b, n) 相似度矩阵\n",
    "        labels: (b,) 正样本位置\n",
    "        \"\"\"\n",
    "        # 提取正样本分数 (b,)\n",
    "        pos_scores = cos_sim.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        print(\"pos_scores:\",pos_scores,\"\\n pos_scores.size:\",pos_scores.size())\n",
    "        # 计算总指数和 (b,)\n",
    "        sum_exp = torch.logsumexp(cos_sim, dim=1).exp()  # 稳定计算\n",
    "        print(\"sum_exp:\",sum_exp,\"\\n sum_exp.size:\",sum_exp.size())\n",
    "        # 计算正样本指数调整项 (b,)\n",
    "        adjusted_pos = pos_scores + torch.log(torch.tensor(self.alpha, device=cos_sim.device))\n",
    "        print(\"adjusted_pos:\",adjusted_pos,\"\\n adjusted_pos.size:\",adjusted_pos.size())\n",
    "        # 分母构造 (alpha*e^pos + sum_neg)\n",
    "        denominator = adjusted_pos.exp() + (sum_exp - pos_scores.exp())\n",
    "        print(\"denominator:\",denominator,\"\\n denominator.size:\",denominator.size())\n",
    "        # 最终损失计算 (b,)\n",
    "        loss = (denominator.log() - pos_scores).mean().item()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc7f02e8-6584-4385-b000-2353a331949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels: tensor([3, 0])\n",
      "pos_scores: tensor([0.3991, 0.0654]) \n",
      " pos_scores.size: torch.Size([2])\n",
      "sum_exp: tensor([10.4917, 10.8044]) \n",
      " sum_exp.size: torch.Size([2])\n",
      "adjusted_pos: tensor([0.8046, 0.4709]) \n",
      " adjusted_pos.size: torch.Size([2])\n",
      "denominator: tensor([11.2369, 11.3383]) \n",
      " denominator.size: torch.Size([2])\n",
      "计算得到的损失值为: 2.1914193630218506\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# 创建损失函数实例\n",
    "criterion = AlphaWeightedCELoss()\n",
    "print(\"cos_sim:\",cos_sim)\n",
    "print(\"nce_labels:\",nce_labels)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(cos_sim, nce_labels)\n",
    "\n",
    "print(f\"计算得到的损失值为: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42806e85-90cc-403f-86c6-60503695e621",
   "metadata": {},
   "source": [
    "#### 添加softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477ab980-63af-45dd-a5a4-d64d43c597c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels: tensor([3, 0])\n",
      "计算得到的损失值为: 2.132995367050171\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlphaWeightedCELoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 确保alpha>0\n",
    "    def forward(self, cos_sim, labels):\n",
    "        \"\"\"\n",
    "        cos_sim: (b, n) 相似度矩阵\n",
    "        labels: (b,) 正样本位置\n",
    "        \"\"\"\n",
    "        cos_sim = torch.softmax(cos_sim, dim=1)  # 普通概率,错误不需要\n",
    "        # 提取正样本分数 (b,)\n",
    "        pos_scores = cos_sim.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        # print(\"pos_scores:\",pos_scores,\"\\n pos_scores.size:\",pos_scores.size())\n",
    "        \n",
    "        # 计算总指数和 (b,)\n",
    "        sum_exp = torch.logsumexp(cos_sim, dim=1).exp()  # 稳定计算\n",
    "        # print(\"sum_exp:\",sum_exp,\"\\n sum_exp.size:\",sum_exp.size())\n",
    "        \n",
    "        # 计算正样本指数调整项 (b,)\n",
    "        adjusted_pos = pos_scores + torch.log(torch.tensor(self.alpha, device=cos_sim.device))\n",
    "        # print(\"adjusted_pos:\",adjusted_pos,\"\\n adjusted_pos.size:\",adjusted_pos.size())\n",
    "        \n",
    "        # 分母构造 (alpha*e^pos + sum_neg)\n",
    "        denominator = adjusted_pos.exp() + (sum_exp - pos_scores.exp())\n",
    "        # print(\"denominator:\",denominator,\"\\n denominator.size:\",denominator.size())\n",
    "        # 最终损失计算 (b,)\n",
    "        loss = (denominator.log() - adjusted_pos).mean().item()\n",
    "        return loss\n",
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# 创建损失函数实例\n",
    "criterion = AlphaWeightedCELoss()\n",
    "print(\"cos_sim:\",cos_sim)\n",
    "print(\"nce_labels:\",nce_labels)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(cos_sim, nce_labels)\n",
    "\n",
    "print(f\"计算得到的损失值为: {loss}\")\n",
    "# cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
    "#         [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
    "# nce_labels: tensor([3, 0])\n",
    "# 计算得到的损失值为: 1.8391039371490479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4227b-93a0-4acb-9c95-dae71499b168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efb6d443-adae-45e6-8e43-53d605b2ce95",
   "metadata": {},
   "source": [
    "#### pytorch 当中封装的 nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c32ed62-3427-47b6-ac23-b91b1c8ee5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels: tensor([3, 0])\n",
      "计算得到的损失值为: 2.13299560546875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "print(\"cos_sim:\",cos_sim)\n",
    "print(\"nce_labels:\",nce_labels)\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(cos_sim, nce_labels)\n",
    "print(f\"计算得到的损失值为: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbea42c-957a-4878-bf07-3c8745388eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c52f80a-2aba-4398-a4e7-cf616d012ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7099)\n",
      "tensor(0.7099)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[[[0.5546, 0.1304, 0.9288],\n",
    "                        [0.6879, 0.3553, 0.9984],\n",
    "                        [0.1474, 0.6745, 0.8948]],\n",
    "                       [[0.8524, 0.2278, 0.6476],\n",
    "                        [0.6203, 0.6977, 0.3352],\n",
    "                        [0.4946, 0.4613, 0.6882]]]])\n",
    "target = torch.tensor([[[0, 0, 0],\n",
    "                       [0, 0, 0],\n",
    "                       [0, 0, 1]]])\n",
    "# cross_entropy的实现               \n",
    "loss = F.cross_entropy(input, target)\n",
    "print(loss)\n",
    "\n",
    "# 利用 nll_loss 实现cross_entropy\n",
    "input = F.softmax(input, dim=1)\n",
    "input = torch.log(input)\n",
    "# input = F.log_softmax(input, dim=1)  # 上面的两行代码和这个是等价的\n",
    "loss = F.nll_loss(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6676f2-ee73-452d-8c13-98349ef271ac",
   "metadata": {},
   "source": [
    "### 融合损失版本的 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd3212fc-0324-4df0-bd18-3f5332f1bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FusedLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, tau=1.0, eps=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 正样本项的加权系数\n",
    "        self.tau = tau      # 温度参数\n",
    "        self.eps = eps      # 数值稳定性常数\n",
    "        # 确保参数合法性\n",
    "        assert self.alpha > 0, \"Alpha must be greater than 0\"\n",
    "        assert self.tau > 0, \"Tau must be greater than 0\"\n",
    "        assert self.eps >= 0, \"Epsilon must be greater than 0\"\n",
    "\n",
    "    def forward(self, cos_sim, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cos_sim: (b, n) 输入的相似度矩阵，未经过温度缩放\n",
    "            labels: (b,) 每个样本的正样本位置索引\n",
    "        Returns:\n",
    "            loss: 计算得到的损失值，保留梯度信息\n",
    "        \"\"\"\n",
    "        print(f\"cos_sim shape: {cos_sim.shape}, values: {cos_sim}\")\n",
    "        print(f\"labels shape: {labels.shape}, values: {labels}\")\n",
    "        device = cos_sim.device\n",
    "        b, n = cos_sim.shape\n",
    "\n",
    "        # 提取正样本分数并增强 (b,)\n",
    "        pos_scores = cos_sim.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        print(f\"pos_scores shape: {pos_scores.shape}, values: {pos_scores}\")\n",
    "        enhanced_pos = self._enhance_pos(pos_scores)  # 正样本增强, φ_m(s_p^i)\n",
    "        print(f\"enhanced_pos shape: {enhanced_pos.shape}, values: {enhanced_pos}\")\n",
    "\n",
    "        # 调制负样本分数 (b, n)\n",
    "        mod_neg_scores = self._modulate_neg(cos_sim)  # 负样本调制, θ_m(s_n^{i,j})\n",
    "        print(f\"mod_neg_scores shape: {mod_neg_scores.shape}, values: {mod_neg_scores}\")\n",
    "        weighted_neg = torch.log(self._weight_neg(cos_sim))  # 负样本加权, log(ψ_m(s_n^{i,j}))\n",
    "        print(f\"weighted_neg shape: {weighted_neg.shape}, values: {weighted_neg}\")\n",
    "        mod_neg_scores_weight = mod_neg_scores + weighted_neg  # 应用对样本进行加权, θ_m(s_n^{i,j}) + log(ψ_m(s_n^{i,j}))\n",
    "        print(f\"mod_neg_scores_weight shape: {mod_neg_scores_weight.shape}, values: {mod_neg_scores_weight}\")\n",
    "\n",
    "        # 生成 mask 以排除正样本位置 (b, n)\n",
    "        mask = torch.zeros_like(mod_neg_scores_weight, dtype=torch.bool)\n",
    "        mask.scatter_(1, labels.unsqueeze(1), True)  # 正样本位置标记为True\n",
    "        print(f\"mask shape: {mask.shape}, values: {mask}\")\n",
    "\n",
    "        # 将正样本位置分数设为-无穷，避免其参与负样本求和\n",
    "        mod_neg_scores_weight_masked = mod_neg_scores_weight.masked_fill(mask, -float('inf'))\n",
    "        print(f\"mod_neg_scores_weight_masked shape: {mod_neg_scores_weight_masked.shape}, values: {mod_neg_scores_weight_masked}\")\n",
    "\n",
    "        # 计算负样本的指数和 (b,)\n",
    "        sum_neg_logits = torch.logsumexp(mod_neg_scores_weight_masked, dim=1)\n",
    "        print(f\"sum_neg_logits shape: {sum_neg_logits.shape}, values: {sum_neg_logits}\")\n",
    "        sum_neg_exp = sum_neg_logits.exp()\n",
    "        print(f\"sum_neg_exp shape: {sum_neg_exp.shape}, values: {sum_neg_exp}\")\n",
    "\n",
    "        # 正样本加权调整 (b,)\n",
    "        weighted_pos = torch.log(self._weight_pos(pos_scores))\n",
    "        print(f\"weighted_pos shape: {weighted_pos.shape}, values: {weighted_pos}\")\n",
    "        adjusted_pos = (enhanced_pos + weighted_pos).exp()\n",
    "        print(f\"adjusted_pos shape: {adjusted_pos.shape}, values: {adjusted_pos}\")\n",
    "\n",
    "        # 构造分母项\n",
    "        denominator = adjusted_pos + sum_neg_exp + self.eps\n",
    "        print(f\"denominator shape: {denominator.shape}, values: {denominator}\")\n",
    "\n",
    "        # 计算最终损失 (log(denominator) - enhanced_pos) 的均值\n",
    "        loss = (torch.log(denominator) - enhanced_pos).mean().item()\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # 定义各调制函数\n",
    "\n",
    "    def _enhance_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本增强函数\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            enhanced_pos: (b,) 增强后的正样本分数\n",
    "        \"\"\"\n",
    "        enhanced_pos = pos_scores.clone().to(pos_scores.device)  # 深拷贝，避免原地操作\n",
    "        enhanced_pos = enhanced_pos/self.tau\n",
    "        return enhanced_pos\n",
    "\n",
    "    def _modulate_neg(self, neg_scores):\n",
    "        \"\"\"\n",
    "        负样本调制函数\n",
    "        Args:\n",
    "            neg_scores: (b, n) 负样本分数\n",
    "        Returns:\n",
    "            modulated_neg: (b, n) 调制后的负样本分数\n",
    "        \"\"\"\n",
    "        modulated_neg = neg_scores.clone().to(neg_scores.device)  # 深拷贝，避免原地操作\n",
    "        modulated_neg = modulated_neg/self.tau\n",
    "        return modulated_neg\n",
    "\n",
    "    def _weight_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本加权函数\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            weighted_pos: (b,) 加权后的正样本分数\n",
    "        \"\"\"\n",
    "        # weighted_pos = pos_scores.clone()\n",
    "        weighted_pos = torch.full_like(pos_scores, self.alpha).to(pos_scores.device)\n",
    "        return weighted_pos\n",
    "\n",
    "    def _weight_neg(self, neg_scores):\n",
    "        \"\"\"\n",
    "        负样本加权函数\n",
    "        Args:\n",
    "            neg_scores: (b, n) 负样本分数\n",
    "        Returns:\n",
    "            weighted_neg: (b, n) 加权后的负样本分数\n",
    "        \"\"\"\n",
    "        # weighted_neg = neg_scores.clone()\n",
    "        weighted_neg = torch.full_like(neg_scores, self.alpha).to(neg_scores.device)\n",
    "        return weighted_neg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae668163-2810-441a-a639-120e1744103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.5, tau=1.0, eps=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 正样本项的加权系数\n",
    "        self.tau = tau      # 温度参数\n",
    "        self.eps = eps      # 数值稳定性常数\n",
    "        # 确保参数合法性\n",
    "        assert self.alpha > 0, \"Alpha must be greater than 0\"\n",
    "        assert self.tau > 0, \"Tau must be greater than 0\"\n",
    "        assert self.eps >= 0, \"Epsilon must be greater than 0\"\n",
    "\n",
    "    def forward(self, cos_sim, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cos_sim: (b, n) 输入的相似度矩阵，未经过温度缩放\n",
    "            labels: (b,) 每个样本的正样本位置索引\n",
    "        Returns:\n",
    "            loss: 计算得到的损失值，保留梯度信息\n",
    "        \"\"\"\n",
    "        device = cos_sim.device\n",
    "        b, n = cos_sim.shape\n",
    "\n",
    "        # 提取正样本分数并增强 (b,)\n",
    "        pos_scores = cos_sim.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        enhanced_pos = self._enhance_pos(pos_scores)  # 正样本增强, φ_m(s_p^i)\n",
    "\n",
    "        # 调制负样本分数 (b, n)\n",
    "        mod_neg_scores = self._modulate_neg(cos_sim)  # 负样本调制, θ_m(s_n^{i,j})\n",
    "        weighted_neg = torch.log(self._weight_neg(cos_sim))  # 负样本加权, log(ψ_m(s_n^{i,j}))\n",
    "        mod_neg_scores_weight = mod_neg_scores + weighted_neg  # 应用对样本进行加权, θ_m(s_n^{i,j}) + log(ψ_m(s_n^{i,j}))\n",
    "\n",
    "        # 生成 mask 以排除正样本位置 (b, n)\n",
    "        mask = torch.zeros_like(mod_neg_scores_weight, dtype=torch.bool)\n",
    "        mask.scatter_(1, labels.unsqueeze(1), True)  # 正样本位置标记为True\n",
    "\n",
    "        # 将正样本位置分数设为-无穷，避免其参与负样本求和\n",
    "        mod_neg_scores_weight_masked = mod_neg_scores_weight.masked_fill(mask, -float('inf'))\n",
    "        \n",
    "        # 计算负样本的指数和 (b,)\n",
    "        sum_neg_logits = torch.logsumexp(mod_neg_scores_weight_masked, dim=1)\n",
    "        sum_neg_exp = sum_neg_logits.exp()\n",
    "\n",
    "        # 正样本加权调整 (b,)\n",
    "        weighted_pos = torch.log(self._weight_pos(pos_scores))  # 正样本加权, log(a_m(s_p^i)) \n",
    "        adjusted_pos = (enhanced_pos + weighted_pos).exp()\n",
    "\n",
    "        # 构造分母项\n",
    "        denominator = adjusted_pos + sum_neg_exp + self.eps\n",
    "\n",
    "        # 计算最终损失 (log(denominator) - enhanced_pos) 的均值\n",
    "        loss = (torch.log(denominator) - enhanced_pos).mean().item()\n",
    "\n",
    "        return loss\n",
    "    # 定义各调制函数\n",
    "\n",
    "    def _enhance_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本增强函数\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            enhanced_pos: (b,) 增强后的正样本分数\n",
    "        \"\"\"\n",
    "        enhanced_pos = pos_scores.clone().to(pos_scores.device)  # 深拷贝，避免原地操作\n",
    "        enhanced_pos = enhanced_pos/self.tau\n",
    "        return enhanced_pos\n",
    "    \n",
    "    def _modulate_neg(self, neg_scores):\n",
    "        \"\"\"\n",
    "        负样本调制函数\n",
    "        Args:\n",
    "            neg_scores: (b, n) 负样本分数\n",
    "        Returns:\n",
    "            modulated_neg: (b, n) 调制后的负样本分数\n",
    "        \"\"\"\n",
    "        modulated_neg = neg_scores.clone().to(neg_scores.device)  # 深拷贝，避免原地操作\n",
    "        modulated_neg = modulated_neg/self.tau\n",
    "        return modulated_neg\n",
    "\n",
    "    def _weight_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本加权函数\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            weighted_pos: (b,) 加权后的正样本分数\n",
    "        \"\"\"\n",
    "        # weighted_pos = pos_scores.clone()\n",
    "        weighted_pos = torch.full_like(pos_scores, self.alpha).to(pos_scores.device)\n",
    "        return weighted_pos\n",
    "    \n",
    "    def _weight_neg(self, neg_scores):\n",
    "        \"\"\"\n",
    "        负样本加权函数\n",
    "        Args:\n",
    "            neg_scores: (b, n) 负样本分数\n",
    "        Returns:\n",
    "            weighted_neg: (b, n) 加权后的负样本分数\n",
    "        \"\"\"\n",
    "        # weighted_neg = neg_scores.clone()\n",
    "        weighted_neg = torch.full_like(neg_scores, 1.0).to(neg_scores.device)\n",
    "        return weighted_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32c884a9-581d-4c0e-8711-97573cecbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels: tensor([3, 0])\n",
      "计算得到的损失值为: 2.1914193630218506\n",
      "计算得到的损失值为: 2.1914193630218506\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# 创建损失函数实例\n",
    "criterion = FusedLoss()\n",
    "print(\"cos_sim:\",cos_sim)\n",
    "print(\"nce_labels:\",nce_labels)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(cos_sim, nce_labels)\n",
    "\n",
    "print(f\"计算得到的损失值为: {loss}\")\n",
    "# 计算得到的损失值为: 2.13299560546875\n",
    "# print(\"计算得到的损失值为: 2.132995367050171\")\n",
    "print(\"计算得到的损失值为: 2.1914193630218506\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5ca9e-cb36-4028-8792-2e451c7ffaa1",
   "metadata": {},
   "source": [
    "### 融合版本2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11b3368-4295-4545-b605-b52d9902e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd0866b-67db-4693-8905-920f097e1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, tau=1.0, eps=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 正样本项的加权系数\n",
    "        self.tau = tau      # 温度参数\n",
    "        self.eps = eps      # 数值稳定性常数\n",
    "        # 确保参数合法性\n",
    "        assert self.alpha > 0, \"Alpha must be greater than 0\"\n",
    "        assert self.tau > 0, \"Tau must be greater than 0\"\n",
    "        assert self.eps >= 0, \"Epsilon must be greater than 0\"\n",
    "\n",
    "    def forward(self, cos_sim, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cos_sim: (b, n) 输入的相似度矩阵，未经过温度缩放\n",
    "            labels: (b,) 每个样本的正样本位置索引\n",
    "        Returns:\n",
    "            loss: 计算得到的损失值，保留梯度信息\n",
    "        \"\"\"\n",
    "        device = cos_sim.device\n",
    "        b, n = cos_sim.shape\n",
    "\n",
    "        # 提取正样本分数并增强 (b,)\n",
    "        pos_scores = cos_sim.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        enhanced_pos = self._enhance_pos(pos_scores)  # 正样本增强, φ_m(s_p^i)\n",
    "\n",
    "        # 生成 mask 以排除正样本位置 (b, n)\n",
    "        mask = torch.zeros_like(cos_sim, dtype=torch.bool).to(device)  # 创建与 cos_sim 相同形状的布尔型张量\n",
    "        mask.scatter_(1, labels.unsqueeze(1), True)                    # 正样本位置标记为True\n",
    "\n",
    "        # 调制负样本分数 (b, n)\n",
    "        mod_neg_scores = self._modulate_neg(cos_sim,mask)  # 负样本调制, θ_m(s_n^{i,j})\n",
    "        weighted_neg = torch.log(self._weight_neg(cos_sim,mask))  # 负样本加权, log(ψ_m(s_n^{i,j}))\n",
    "        mod_neg_scores_weight = mod_neg_scores + weighted_neg  # 应用对样本进行加权, θ_m(s_n^{i,j}) + log(ψ_m(s_n^{i,j}))\n",
    "\n",
    "        # 将正样本位置分数设为-无穷，避免其参与负样本求和\n",
    "        mod_neg_scores_weight_masked = mod_neg_scores_weight.masked_fill(mask, -float('inf'))\n",
    "        \n",
    "        # # 计算负样本的指数和 (b,),将精度转换成 float，因为 logsumexp 不支持 bf16\n",
    "        # sum_neg_logits = torch.logsumexp(mod_neg_scores_weight_masked.float(), dim=1).to(mod_neg_scores_weight_masked.dtype)  # 转回原类型\n",
    "        # sum_neg_exp = sum_neg_logits.exp()\n",
    "        \n",
    "        # 计算负样本的指数和 (b,)\n",
    "        sum_neg_logits = torch.exp(mod_neg_scores_weight_masked)\n",
    "        sum_neg_exp = torch.sum(sum_neg_logits, dim=1)\n",
    "\n",
    "        # 正样本加权调整 (b,)\n",
    "        weighted_pos = torch.log(self._weight_pos(pos_scores))  # 正样本加权, log(a_m(s_p^i)) \n",
    "        adjusted_pos = (enhanced_pos + weighted_pos).exp()\n",
    "\n",
    "        # 构造分母项\n",
    "        denominator = adjusted_pos + sum_neg_exp + self.eps\n",
    "\n",
    "        # 计算最终损失 (log(denominator) - enhanced_pos) 的均值\n",
    "        loss = (torch.log(denominator) - enhanced_pos).mean()\n",
    "\n",
    "        return loss\n",
    "    # 定义各调制函数\n",
    "\n",
    "    def _enhance_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本增强函数\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            enhanced_pos: (b,) 增强后的正样本分数\n",
    "        \"\"\"\n",
    "        enhanced_pos = pos_scores.clone().to(pos_scores.device)  # 深拷贝，避免原地操作\n",
    "        enhanced_pos = enhanced_pos/self.tau\n",
    "        return enhanced_pos\n",
    "    \n",
    "    def _modulate_neg(self, cos_sim, mask=None):\n",
    "        \"\"\"\n",
    "        负样本调制函数\n",
    "        Args:\n",
    "            cos_sim: (b, n) 相似度矩阵\n",
    "        Returns:\n",
    "            modulated_neg: (b, n) 调制后的负样本分数\n",
    "        \"\"\"\n",
    "        modulated_neg = cos_sim.clone().to(cos_sim.device)  # 深拷贝，避免原地操作\n",
    "        modulated_neg = modulated_neg/self.tau\n",
    "        return modulated_neg\n",
    "\n",
    "    def _weight_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本加权函数\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            weighted_pos: (b,) 加权后的正样本分数\n",
    "        \"\"\"\n",
    "        # weighted_pos = pos_scores.clone()\n",
    "        weighted_pos = torch.full_like(pos_scores, self.alpha).to(pos_scores.device)\n",
    "        return weighted_pos\n",
    "    \n",
    "    def _weight_neg(self, cos_sim, mask=None):\n",
    "        \"\"\"\n",
    "        负样本加权函数\n",
    "        Args:\n",
    "            cos_sim: (b, n) 相似度矩阵\n",
    "        Returns:\n",
    "            weighted_neg: (b, n) 加权后的负样本分数\n",
    "        \"\"\"\n",
    "        # weighted_neg = cos_sim.clone()\n",
    "        weighted_neg = torch.full_like(cos_sim, self.alpha).to(cos_sim.device)\n",
    "        return weighted_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa5e311-2677-4273-b495-51268353f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels: tensor([3, 0])\n",
      "计算得到的损失值为: 2.132995367050171\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# 创建损失函数实例\n",
    "criterion = FusedLoss()\n",
    "print(\"cos_sim:\",cos_sim)\n",
    "print(\"nce_labels:\",nce_labels)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(cos_sim, nce_labels)\n",
    "\n",
    "print(f\"计算得到的损失值为: {loss}\")\n",
    "# 计算得到的损失值为: 2.13299560546875\n",
    "# print(\"计算得到的损失值为: 2.132995367050171\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0534162-0efb-4bd5-93b7-4350026f2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalInfoNCELoss(FusedLoss):\n",
    "    def __init__(self, alpha=1.0, tau=0.05, eps=1e-8,margin=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: 权重参数\n",
    "            tau: 温度参数\n",
    "            eps: 数值稳定性参数\n",
    "            margin: 边界参数\n",
    "        \"\"\"\n",
    "        self.margin = margin\n",
    "        print(f\"FocalInfoNCELoss: alpha={alpha}, tau={tau}, eps={eps}, margin={margin}\")\n",
    "        super().__init__(alpha, tau, eps)\n",
    "    \n",
    "    def _enhance_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本增强函数,φ_m(s_p^i) = (s_p^i)^2/τ\n",
    "        这里的正样本增强是平方操作\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            enhanced_pos: (b,) 增强后的正样本分数\n",
    "        \"\"\"\n",
    "        enhanced_pos = pos_scores.clone().to(pos_scores.device)\n",
    "        enhanced_pos = enhanced_pos.pow(2)\n",
    "        enhanced_pos = enhanced_pos/self.tau\n",
    "        return enhanced_pos\n",
    "    def _modulate_neg(self, cos_sim,mask=None):\n",
    "        \"\"\"\n",
    "        负样本调制函数,θ_m(s_n^{i,j}) = (s_n^{i,j})(s_n^{i,j}+margin)/τ\n",
    "        这里的负样本调制是带margin的平方操作\n",
    "        Args:\n",
    "            cos_sim: (b, n) 相似度矩阵\n",
    "        Returns:\n",
    "            modulated_neg: (b, n) 调制后的负样本分数\n",
    "        \"\"\"\n",
    "        modulated_neg = cos_sim.clone().to(cos_sim.device)\n",
    "        modulated_neg = modulated_neg*(modulated_neg + self.margin)\n",
    "        modulated_neg = modulated_neg/self.tau\n",
    "        return modulated_neg\n",
    "    def _weight_pos(self, pos_scores):\n",
    "        \"\"\"\n",
    "        正样本加权函数, a_m(s_p^i) = alpha;\n",
    "        这里的正样本加权是 alpha\n",
    "        Args:\n",
    "            pos_scores: (b,) 正样本分数\n",
    "        Returns:\n",
    "            weighted_pos: (b,) 加权后的正样本分数\n",
    "        \"\"\"\n",
    "        # weighted_pos = pos_scores.clone()\n",
    "        weighted_pos = torch.full_like(pos_scores, self.alpha).to(pos_scores.device)\n",
    "        return weighted_pos\n",
    "    def _weight_neg(self, cos_sim, mask=None):\n",
    "        \"\"\"\n",
    "        负样本加权函数, ψ_m(s_n^{i,j}) = alpha;\n",
    "        这里的负样本加权是 alpha\n",
    "        Args:\n",
    "            cos_sim: (b, n) 相似度矩阵\n",
    "        Returns:\n",
    "            weighted_neg: (b, n) 加权后的负样本分数\n",
    "        \"\"\"\n",
    "        # weighted_neg = cos_sim.clone()\n",
    "        weighted_neg = torch.full_like(cos_sim, self.alpha).to(cos_sim.device)\n",
    "        return weighted_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f30b346-00e5-4ff7-a094-9e480c7f9b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FocalInfoNCELoss: alpha=1.0, tau=0.05, eps=1e-08, margin=0.3\n",
      "cos_sim: tensor([[0.6850, 0.9355, 0.2900, 0.3991, 0.7470, 0.0215],\n",
      "        [0.0654, 0.7855, 0.3883, 0.6340, 0.9447, 0.4773]])\n",
      "nce_labels: tensor([3, 0])\n",
      "计算得到的损失值为: 21.682632446289062\n",
      "计算得到的损失值为: 2.1914193630218506\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟数据\n",
    "torch.manual_seed(2025)\n",
    "batch_size = 2\n",
    "num_classes = 3*batch_size\n",
    "cos_sim = torch.rand(batch_size, num_classes)\n",
    "nce_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# 创建损失函数实例\n",
    "criterion = FocalInfoNCELoss()\n",
    "print(\"cos_sim:\",cos_sim)\n",
    "print(\"nce_labels:\",nce_labels)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(cos_sim, nce_labels)\n",
    "\n",
    "print(f\"计算得到的损失值为: {loss}\")\n",
    "# 计算得到的损失值为: 2.13299560546875\n",
    "# print(\"计算得到的损失值为: 2.132995367050171\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb77fda-9d6f-475a-a400-6308ff7e79df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab8dca-47af-47dc-b3a5-69eb16f5f554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1af124-950c-4810-ad30-bae7901d9362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcf89ed-eb5e-4639-a8fc-3041c464d0ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-14eee4b839fe>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 检查所有元素是否在 -1 到 1 之间\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例张量矩阵\n",
    "tensor = torch.tensor([[1.5, -0.3], [0.8, 0.9]])\n",
    "\n",
    "# 检查所有元素是否在 -1 到 1 之间\n",
    "result = torch.all((tensor >= -1) & (tensor <= 1))\n",
    "assert result == (True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffb2da-324d-4978-b0dc-844ae1055e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(3.8.8)",
   "language": "python",
   "name": "env-3.8.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
